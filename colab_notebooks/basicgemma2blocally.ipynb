{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/vijaysrajan/buysellconnect/blob/main/BasicGemma2bLocally.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"NxS_YI5xb9lT"},"outputs":[],"source":["# Step 1: Install required packages with compatible versions\n","!pip install --upgrade transformers>=4.38.0\n","!pip install --upgrade torch torchvision torchaudio\n","!pip install --upgrade accelerate\n","!pip install langchain>=0.1.0\n","!pip install langchain-huggingface\n","!pip install huggingface_hub>=0.20.0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYEp0dVaKEab","collapsed":true},"outputs":[],"source":["!pip install langchain-community"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCAzeZKdcAvb"},"outputs":[],"source":["\n","# Step 2: Restart runtime after installation (important!)\n","# Go to Runtime > Restart runtime in Colab, then run the cells below\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVUn317VcEqE"},"outputs":[],"source":["\n","# Step 3: Authenticate with Hugging Face (Gemma is a gated model)\n","from huggingface_hub import notebook_login\n","notebook_login()  # This will prompt you to enter your HF token\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_EsdSpgcGan"},"outputs":[],"source":["\n","# Step 4: Import required libraries\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","from langchain_huggingface import HuggingFacePipeline\n","from langchain.schema import HumanMessage\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain_community.embeddings import HuggingFaceEmbeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yc9K3FOecIMO"},"outputs":[],"source":["\n","# Step 5: Check if GPU is available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZxiwWuUcJ78"},"outputs":[],"source":["\n","# Step 6: Load the model and tokenizer locally\n","model_id = \"google/gemma-2b-it\"\n","\n","print(\"Loading tokenizer...\")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","print(\"Loading model...\")\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n","    device_map=\"auto\" if device == \"cuda\" else None,\n","    low_cpu_mem_usage=True\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-i6u3KpTcLV2"},"outputs":[],"source":["\n","# # Detect if CUDA is available and set device\n","# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# # Step 7: Create a pipeline\n","# text_generation_pipeline = pipeline(\n","#     \"text-generation\",\n","#     model=model,\n","#     tokenizer=tokenizer,\n","#     max_new_tokens=1000,\n","#     do_sample=False,  # Greedy decoding\n","#     return_full_text=False,\n","#     device=1 if device == \"cuda\" else -1\n","# )\n","\n","\n","\n","\n","# Detect if CUDA is available and set device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device=\"auto\"\n","# Step 7: Create a pipeline\n","text_generation_pipeline = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_new_tokens=1000,\n","    do_sample=False,  # Greedy decoding\n","    return_full_text=False,\n","    temperature=0.001,\n","    #device=0 if device == \"cuda\" else -1  # Use GPU 0 if available, else CPU\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qD02Ed5ccNDS"},"outputs":[],"source":["# Step 8: Create LangChain wrapper\n","llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n","\n","# Step 9: Test the model\n","print(\"Testing the model...\")\n","#prompt = \"Explain what is Generative AI in 3 bullet points\"\n","prompt = \"\"\" \\\n","### Input: \\\n","I want to buy my 2 year old fridge. It is 350 litres and red in colour. I am moving out of my town. \\\n","### Schema:\\\n","{\n","  \"item\": \"string\",\n","  \"quantity\": \"string\",\n","  \"colour\": \"string\"\n","  \"capacity\": \"string\"\\\n","} \\\n","### Response: \\\n","\"\"\"\n","\n","\n","response = llm.invoke(prompt)\n","print(\"Response:\")\n","print(response)\n","\n","# Alternative: Direct pipeline usage (without LangChain)\n","print(\"\\n\" + \"=\"*50)\n","print(\"Alternative: Direct pipeline usage\")\n","result = text_generation_pipeline(prompt)\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qQAmJ1jH3xs"},"outputs":[],"source":["Q1 = \"I want to sell my 2 year old fridge. It is 350 litres and red in colour. I am moving out of my town.\"\n","\n","\n","Q = Q1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPF-LJZXHaFg"},"outputs":[],"source":["prompt_template1 = ChatPromptTemplate.from_template(f\"\"\"\n","<bos><start_of_turn>user\n","Answer the following question based on the provided context. If you cannot answer the question based on the context, say \"I don't have enough information to answer that question.\"\n","\n","Context: Is the question from a buyer or seller or a lessor or renter or is the question from someone who wants to connect. Only 5 choices possible buyer, seller, connector, lessor, renter. Please print only one of the above choices and print that. Do not print anything else.\n","\n","Question: {Q}\n","<end_of_turn>\n","<start_of_turn>model\n","\"\"\")\n","print (prompt_template1.format_messages(Q=Q))"]},{"cell_type":"code","source":["prompt_template2 = ChatPromptTemplate.from_template(f\"\"\"\n","<bos><start_of_turn>user\n","Classify the following statement into exactly ONE category:\n","- buyer\n","- seller\n","- connector\n","- lessor\n","- renter\n","\n","Statement: {Q}\n","\n","Respond with ONLY the category name. No explanation, no additional text, just the single word.\n","<end_of_turn>\n","<start_of_turn>model\n","\"\"\")\n","print (prompt_template2.format_messages(Q=Q))"],"metadata":{"id":"C7ugcFmnzPpQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p-0Z-cCkDzof"},"outputs":[],"source":["prompt_template3 = ChatPromptTemplate.from_template(f\"\"\"\n","<bos><start_of_turn>user\n","Answer the following question based on the provided context in one word or phrase. If you cannot answer the question based on the context, say \"I don't have enough information to answer that question.\" else do not print any context in the response. I am looking for a single word or phrase that best answers the question.\n","\n","Context: Please extract the attributes and features like colour, age, make and model, capacity etc if available, in the question and put it in a json format. Please start with the item under consideration followed by the attributes.\n","\n","Question: {Q}\n","<end_of_turn>\n","<start_of_turn>model\n","\"\"\")\n","\n","print (prompt_template3.format_messages(Q=Q))"]},{"cell_type":"code","source":["prompt_template4 = ChatPromptTemplate.from_template(f\"\"\"\n","<bos><start_of_turn>user\n","For the question:{Q}, please fill the following json appropriately. Please only output the json. Please fill json values only where applicable or fill null.\n","\n","{{{{\n","  \"question_raiser_type\": \"string\", #buyer or seller or connector or lessor or renter\n","  \"item\": \"string\", #no adjectives. no descriptions. Just the item no description single word or phrase only. Only item name. If applicable else null.\n","  \"item_category\": \"string\",\n","  \"quantity\": \"string\",\n","  \"colour\": \"string\",\n","  \"capacity\": \"string\",\n","  \"age\": \"string\",\n","  \"truck_type\": \"string\",\n","}}}}\n","<end_of_turn>\n","<start_of_turn>model\"\"\")\n","\n","# Removed the incorrect print statement\n","print (prompt_template4.format_messages(Q=Q))"],"metadata":{"id":"60rJl-BH4l5W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def post_processing(response):\n","  # Extract just the classification word\n","  return_word = \"\"\n","  classification_words = [\"buyer\", \"seller\", \"connector\", \"lessor\", \"renter\", \"service_provider\", \"service_seeker\"]\n","  for word in classification_words:\n","      if word.lower() in response.lower():\n","          return_word = word\n","          #print(word)\n","          break\n","  else:\n","      print(response.strip())  # Fallback to original if no match found\n","  return return_word"],"metadata":{"id":"nFCvMw3L0jpU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a83b757f"},"outputs":[],"source":["qa_rag_chain = (\n","    # Removed commented out code and fixed syntax\n","    {\"question\": RunnablePassthrough()}\n","    | prompt_template4\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# Assuming 'question' variable is defined elsewhere or should be Q\n","# Using Q based on previous cell execution\n","response = qa_rag_chain.invoke({\"question\": Q})\n","print(response)\n","print(post_processing(response))\n"]},{"cell_type":"code","source":[],"metadata":{"id":"McCIq8Ov0Gs9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5e4a84ae"},"source":["**Note:** You need to restart the runtime after installing the library for the changes to take effect. Go to `Runtime > Restart runtime` in the Colab menu."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}